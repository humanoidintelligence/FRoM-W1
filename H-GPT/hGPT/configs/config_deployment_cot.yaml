SEED_VALUE: 1234 # Seed value
DEBUG: True # Debug mode

NAME: 0428-llama-cotv3-train-padding-true-demo # Experiment names
ACCELERATOR: 'gpu' # Devices optioncal: “cpu”, “gpu”, “tpu”, “ipu”, “hpu”, “mps, “auto”
NUM_NODES: 1 # Number of GPU nodes for distributed training
DEVICE: [0] # Index of gpus eg. [0] or [0,1,2,3] [0,1,2,3,4,5,6,7], "auto"

DATASET:
  target: hGPT.data.MotionX.MotionXDataModule
  NFEATS: 623
  WORD_VERTILIZER_PATH: deps/glove/
  TASK_PATH: 'datasets/motionx/data/instructions/template_pretrain_cot.json'
  MOTIONX:
    NJOINTS: 52
    SPLIT_PATH: datasets/motionx/data/split
    MOTION_FEAT_PATH: datasets/motionx/data/motion_data/vectors_${DATASET.NFEATS}
    SEMANTIC_TEXT_PATH: datasets/motionx/data/texts/semantic_labels
    COT_PATH: 'datasets/motionx/data/texts/cot/v3'
    MEAN_STD_PATH: datasets/motionx/data/mean_std/vectors_${DATASET.NFEATS}
    MIN_MOTION_LEN: 40
    MAX_MOTION_LEN: 400
    UNIT_LEN: 4
    FRAME_RATE: 30.0
    VAE_WIN_SIZE: 64
    MOTION_TOKEN_PATH: 'datasets/motionx/data/TOKENS'
    MAX_TEXT_LEN: 20
    STD_TEXT: False

model:
  target: hGPT.models.hgpt.HumanoidGPT
  params:
    stage: ${TRAIN.STAGE}
    task: 't2m'
    lm: ${lm.llama}
    motion_vae: ${vq.vqvae_2k_1k}
    codebook_size: ${model.params.motion_vae.params.code_num}
    condition: 'text'
    metrics_dict: ${METRIC.TYPE}
    debug: ${DEBUG} 
    
TRAIN:
  #---------------------------------
  STAGE: lm_pretrain # stage "vae" , "lm_pretrain", "lm_instruct"
  SPLIT: 'train' # Training split name
  PRECISION: 'bf16-mixed' # PRECISION: 'bf16-mixed'
  STRATEGY: 'deepspeed_stage_2'
  #---------------------------------
  NUM_WORKERS: 16 # Number of workers, 16 before
  BATCH_SIZE: 2 # Size of batches, 16 before
  ACCUMULATE_GRAD_BATCHES: 8 # 8 before
  END_EPOCH: 300 # End epoch
  RESUME: '' # Resume training from this path
  PRETRAINED: '' # Preatrained model path
  PRETRAINED_VAE: 'YOUR_PATH_TO_TOKENIZER' # checkpoints/MotionGPT-base/motiongpt_s3_h3d.tar # Vae model path

  OPTIM:
    target: AdamW
    params:
      lr: 2e-4 # 2e-5 before
      betas: [0.9, 0.99]
      weight_decay: 0.0

  LR_SCHEDULER:
    target: CosineAnnealingLR
    params:
      T_max: ${eval:${LOGGER.VAL_EVERY_STEPS} * 100}
      eta_min: 1e-6

# Evaluating Configuration
EVAL:
  SPLIT: val
  BATCH_SIZE: 2 # Evaluating Batch size, 32 before
  NUM_WORKERS: 8 # Validation Batch size, 8 before

TEST:
  DATASETS: ['MotionX']
  CHECKPOINTS: 'YOUR_PATH_TO_MOTION_GENERATOR'
  SPLIT: test
  BATCH_SIZE: 1 # training Batch size, 8, 32 before
  NUM_WORKERS: 16

  SAVE_PREDICTIONS: True # Weather to save predictions
  COUNT_TIME: True # Weather to count time during test
  REPLICATION_TIMES: 1 # Number of times to replicate the test, 20 before
  REP_I: 0 # For counting replication times

LOSS:
  LAMBDA_FEATURE: 1.0
  LAMBDA_VELOCITY: 0.5
  LAMBDA_COMMIT: 0.02
  LAMBDA_CLS: 1.0
  ABLATION:
    RECONS_LOSS: 'l1_smooth'

METRIC:
  TYPE: ['TM2TMetrics', 'PredMetrics']
  FORCE_IN_METER: True
  DIST_SYNC_ON_STEP: True
  MM_NUM_SAMPLES: 100 # Number of samples for multimodal test
  MM_NUM_REPEATS: 30 # Number of repeats for multimodal test
  MM_NUM_TIMES: 10 # Number of times to repeat the multimodal test
  DIVERSITY_TIMES: 300 # Number of times to repeat the diversity test
  TM2T: ${eval.tm2t}

LOGGER:
  VAL_EVERY_STEPS: 100 # 10 before, 100 for debug.
  TYPE: ['wandb']
  WANDB:
    target: pytorch_lightning.loggers.WandbLogger
    params:
      project: humanoidgpt
      offline: True
      id: null
      version: ''
      name: ${NAME}
      save_dir: ${FOLDER_EXP}
